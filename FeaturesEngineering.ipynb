{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import igraph\n",
    "import networkx as nx\n",
    "from tqdm import tqdm_notebook,tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_data = \"data/\"\n",
    "\n",
    "training_set = pd.read_table(path_to_data+\"training_set.txt\",sep=\" \",names=[\"source\",\"target\",\"label\"])\n",
    "testing_set = pd.read_table(path_to_data+\"testing_set.txt\",sep=\" \",names=[\"source\",\"target\"])\n",
    "node_info = pd.read_csv(path_to_data+\"node_information.csv\",names=[\"id\",\"year\",\"title\",\"author\",\"classification\",\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_notebook().pandas(\"Desc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "N_VECTORIZATION = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=stpwds,ngram_range=(1,3),max_features=2000,norm=\"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorized_desc = tfidf.fit_transform(list(node_info.description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [element.split(\" \") for element in node_info[\"description\"]]\n",
    "for i in tqdm_notebook(range(len(corpus))):\n",
    "    corpus[i] = [stemmer.stem(el) for el in corpus[i] if (el.isdigit()==False and len(el)>2 and len(el)<15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "docs = []\n",
    "\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "for i, words in tqdm_notebook(enumerate(corpus)):\n",
    "    tags = [i]\n",
    "    docs.append(analyzedDocument(words, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(docs, size=N_VECTORIZATION, window=8, min_count=5, workers=4)\n",
    "liste = [list(model.docvecs[i]) for i in tqdm_notebook(range(len(model.docvecs)))]\n",
    "\n",
    "node_info = node_info.merge(pd.DataFrame(liste, columns=[\"description_d2v_\"+str(i) for i in range(N_VECTORIZATION)]),how='outer', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=stpwds,ngram_range=(1,3),max_features=2000,norm=\"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorized_title = tfidf.fit_transform(list(node_info.title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [element.split(\" \") for element in node_info[\"title\"]]\n",
    "for i in tqdm_notebook(range(len(corpus))):\n",
    "    corpus[i] = [stemmer.stem(el) for el in corpus[i] if (el.isdigit()==False and len(el)>2 and len(el)<15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "docs = []\n",
    "\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "for i, words in tqdm_notebook(enumerate(corpus)):\n",
    "    tags = [i]\n",
    "    docs.append(analyzedDocument(words, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(docs, size=N_VECTORIZATION, window=8, min_count=5, workers=4)\n",
    "liste = [list(model.docvecs[i]) for i in range(len(model.docvecs))]\n",
    "\n",
    "node_info = node_info.merge(pd.DataFrame(liste, columns=[\"title_d2v_\"+str(i) for i in range(N_VECTORIZATION)]),how='outer', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where training and testing set receive clean data from node_info to optimize the building of useful features later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_name(val):\n",
    "    if val!=val:\n",
    "        return \"\"\n",
    "    ar = val.split(\",\")\n",
    "    for i in range(len(ar)):\n",
    "        if ar[i][0]== \" \" and len(ar[i])>1: #get rid of the first space\n",
    "            ar[i] = ar[i][1:]\n",
    "        ar[i] = \";\".join(ar[i].split(\" \"))\n",
    "        #or ar[i] = ar[i].split(\" \")[-1]\n",
    "        ar[i] = ar[i].replace(\"(\", \"\")\n",
    "        ar[i] = ar[i].replace(\")\", \"\")\n",
    "    return \" \".join(ar)\n",
    "\n",
    "node_info[\"author\"] = node_info[\"author\"].progress_apply(lambda val: clear_name(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node_info[\"pos\"] = pd.Series([i for i in range(len(node_info))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = training_set.merge(node_info, how=\"left\", left_on = \"source\", right_on = \"id\")\n",
    "training_set = training_set.merge(node_info, how=\"left\", left_on = \"target\", right_on = \"id\", suffixes= [\"_source\",\"_target\"])\n",
    "\n",
    "testing_set = testing_set.merge(node_info, how=\"left\", left_on = \"source\", right_on = \"id\")\n",
    "testing_set = testing_set.merge(node_info, how=\"left\", left_on = \"target\", right_on = \"id\", suffixes= [\"_source\",\"_target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(tfidf, corpus, model, analyzedDocument, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features based on graph topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edges = training_set[training_set[\"label\"]==1]\n",
    "\n",
    "ig = igraph.Graph()\n",
    "ig.add_vertices(node_info.id)\n",
    "ig.add_edges([(source,target) for source,target in zip(edges.pos_source,edges.pos_target)])\n",
    "\n",
    "g = nx.Graph()\n",
    "g.add_nodes_from(node_info.id)\n",
    "g.add_edges_from([(source,target) for source,target in zip(edges.source,edges.target)])\n",
    "\n",
    "dg = nx.DiGraph()\n",
    "dg.add_nodes_from(node_info.id)\n",
    "dg.add_edges_from([(source,target) for source,target in zip(edges.source,edges.target)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_neighbor(line):\n",
    "    n_source = set(ig.neighbors(line[\"pos_source\"], mode=\"ALL\"))\n",
    "    n_target = set(ig.neighbors(line[\"pos_target\"], mode=\"ALL\"))\n",
    "    return(len(n_source & n_target)>0)\n",
    "\n",
    "training_set[\"common_neighbor\"] = training_set.progress_apply(common_neighbor,axis=1)\n",
    "testing_set[\"common_neighbor\"] = testing_set.progress_apply(common_neighbor,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(line):\n",
    "    n_source = set(ig.neighbors(line[\"pos_source\"], mode=\"ALL\"))\n",
    "    n_target = set(ig.neighbors(line[\"pos_target\"], mode=\"ALL\"))\n",
    "    if(len(n_source) == 0 and len(n_target)==0):\n",
    "        return 1\n",
    "    return(len(n_source & n_target)/len(n_source | n_target))\n",
    "\n",
    "training_set[\"jaccard\"] = training_set.progress_apply(jaccard,axis=1)\n",
    "testing_set[\"jaccard\"] = testing_set.progress_apply(jaccard,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenness = ig.betweenness()\n",
    "\n",
    "training_set[\"betweenness_source\"] = training_set[\"pos_source\"].progress_apply(lambda x : betweenness[x])\n",
    "testing_set[\"betweenness_source\"] = testing_set[\"pos_source\"].progress_apply(lambda x : betweenness[x])\n",
    "\n",
    "training_set[\"betweenness_target\"] = training_set[\"pos_target\"].progress_apply(lambda x : betweenness[x])\n",
    "testing_set[\"betweenness_target\"] = testing_set[\"pos_target\"].progress_apply(lambda x : betweenness[x])\n",
    "\n",
    "training_set[\"diff_in_bc\"] = training_set[\"betweenness_target\"] - training_set[\"betweenness_source\"]\n",
    "testing_set[\"diff_in_bc\"] = testing_set[\"betweenness_target\"] - testing_set[\"betweenness_source\"]\n",
    "\n",
    "del(betweenness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inlinks = ig.indegree()\n",
    "\n",
    "def diff_in_inlinks(line):\n",
    "    return(inlinks[line[\"pos_target\"]]-inlinks[line[\"pos_source\"]])\n",
    "\n",
    "training_set[\"diff_in_inlinks\"] = training_set.progress_apply(diff_in_inlinks,axis=1)\n",
    "testing_set[\"diff_in_inlinks\"] = testing_set.progress_apply(diff_in_inlinks,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster = connected component\n",
    "\n",
    "cluster = ig.clusters().membership\n",
    "\n",
    "def same_cluster(line):\n",
    "    return(cluster[line[\"pos_target\"]]-cluster[line[\"pos_source\"]])\n",
    "\n",
    "training_set[\"same_cluster\"] = training_set.progress_apply(same_cluster,axis=1)\n",
    "testing_set[\"same_cluster\"] = testing_set.progress_apply(same_cluster,axis=1)\n",
    "\n",
    "del(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eccentricity : cumulative sum of the distance to other edges\n",
    "\n",
    "eccentricity = ig.eccentricity(mode=\"IN\")\n",
    "\n",
    "def target_eccentricty(line):\n",
    "    return(eccentricity[line[\"pos_target\"]])\n",
    "\n",
    "training_set[\"target_eccentricty\"] = training_set.progress_apply(target_eccentricty,axis=1)\n",
    "testing_set[\"target_eccentricty\"] = testing_set.progress_apply(target_eccentricty,axis=1)\n",
    "\n",
    "del(eccentricity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Distance\n",
    "dijsktra = nx.all_pairs_dijkstra_path_length(dg, cutoff=5)\n",
    "\n",
    "def shortest_path(line):\n",
    "    try:\n",
    "        return(dijsktra[line[\"source\"]][line[\"target\"]])\n",
    "    except:\n",
    "        return(6)\n",
    "\n",
    "training_set[\"shortest_path\"] = training_set.progress_apply(shortest_path,axis=1)\n",
    "testing_set[\"shortest_path\"] = testing_set.progress_apply(shortest_path,axis=1)\n",
    "\n",
    "del(dijsktra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features based on graph topology for the author graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creation graph for author -> two authors are linked if one cites another\n",
    "auth_list = []\n",
    "\n",
    "for author_names in node_info[\"author\"]:\n",
    "    for name in author_names.split(\"_\"):\n",
    "        auth_list.append(name)\n",
    "        \n",
    "auth_list = set(auth_list)\n",
    "auth_list.remove(\"\")\n",
    "auth_list = list(auth_list)\n",
    "\n",
    "author_to_index = dict(zip(auth_list, range(len(auth_list))))\n",
    "\n",
    "g_author = igraph.Graph()\n",
    "g_author.add_vertices(len(auth_list))\n",
    "\n",
    "lst_edges = []\n",
    "\n",
    "auth_src_l, auth_tgt_l = training_set[\"author_source\"], training_set[\"author_target\"]\n",
    "for i in range(len(auth_src_l)):\n",
    "    auth_src = auth_src_l[i]\n",
    "    auth_tgt = auth_tgt_l[i]\n",
    "    \n",
    "    if auth_src != \"\" and auth_tgt != \"\":\n",
    "        auth_src = auth_src.split(\"_\")\n",
    "        auth_tgt = auth_tgt.split(\"_\")\n",
    "        for name_scr in auth_src:\n",
    "            for name_tgt in auth_tgt:\n",
    "                if name_scr != \"\" and name_tgt!=\"\" and not g_author.are_connected(author_to_index[name_scr], author_to_index[name_tgt]):\n",
    "                    lst_edges.append((author_to_index[name_scr], author_to_index[name_tgt]))\n",
    "\n",
    "lst_edges = list(set(lst_edges))\n",
    "g_author.add_edges(lst_edges)    \n",
    "print(\"author graph created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "betweenness = g_author.betweenness()\n",
    "\n",
    "def auth_betweenness(elt):\n",
    "    auth_list = []\n",
    "    for name in elt.split(\"_\"):\n",
    "        auth_list.append(name)\n",
    "    l = []\n",
    "    for auth in auth_list:\n",
    "        if auth != \"\" and auth in author_to_index:\n",
    "            l.append(betweenness[author_to_index[auth]])\n",
    "        else:\n",
    "            l.append(0)\n",
    "    return max(l)\n",
    "\n",
    "training_set[\"betweenness_author_target\"] = training_set[\"author_target\"].progress_apply(auth_betweenness)\n",
    "testing_set[\"betweenness_author_target\"] = testing_set[\"author_target\"].progress_apply(auth_betweenness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inlinks = g_author.indegree()\n",
    "\n",
    "def auth_inlinks(elt):\n",
    "    auth_list = []\n",
    "    for name in elt.split(\"_\"):\n",
    "        auth_list.append(name)\n",
    "    l = []\n",
    "    for auth in auth_list:\n",
    "        if auth != \"\" and auth in author_to_index:\n",
    "            l.append(inlinks[author_to_index[auth]])\n",
    "        else:\n",
    "            l.append(0)\n",
    "    return max(l)\n",
    "\n",
    "training_set[\"inlinks_author_target\"] = training_set[\"author_target\"].progress_apply(auth_inlinks)\n",
    "testing_set[\"inlinks_author_target\"] = testing_set[\"author_target\"].progress_apply(auth_inlinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cos similarity in Doc2Vec title\n",
    "\n",
    "def cos_similarity_title(line):\n",
    "    scal = vectorized_title[line[\"pos_source\"],:].dot(vectorized_title[line[\"pos_target\"],:].T)[0,0]\n",
    "    source = vectorized_title[line[\"pos_source\"],:].dot(vectorized_title[line[\"pos_source\"],:].T)[0,0]\n",
    "    target = vectorized_title[line[\"pos_target\"],:].dot(vectorized_title[line[\"pos_target\"],:].T)[0,0]\n",
    "    return(scal/np.sqrt(target*source))\n",
    "\n",
    "training_set[\"cos_similarity_title\"] = training_set.progress_apply(cos_similarity_title,axis=1)\n",
    "testing_set[\"cos_similarity_title\"] = testing_set.progress_apply(cos_similarity_title,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cos similarity in Doc2Vec description\n",
    "\n",
    "def cos_similarity_description(line):\n",
    "    scal = vectorized_desc[line[\"pos_source\"],:].dot(vectorized_desc[line[\"pos_target\"],:].T)[0,0]\n",
    "    source = vectorized_desc[line[\"pos_source\"],:].dot(vectorized_desc[line[\"pos_source\"],:].T)[0,0]\n",
    "    target = vectorized_desc[line[\"pos_target\"],:].dot(vectorized_desc[line[\"pos_target\"],:].T)[0,0]\n",
    "    return(scal/np.sqrt(target*source))\n",
    "\n",
    "training_set[\"cos_similarity_desc\"] = training_set.progress_apply(cos_similarity_description,axis=1)\n",
    "testing_set[\"cos_similarity_desc\"] = testing_set.progress_apply(cos_similarity_description,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diff_in_year(line):\n",
    "    return(line[\"year_target\"]-line[\"year_source\"])\n",
    "\n",
    "training_set[\"diff_in_year\"] = training_set.progress_apply(diff_in_year,axis=1)\n",
    "testing_set[\"diff_in_year\"] = testing_set.progress_apply(diff_in_year,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nb_common_author(line):\n",
    "    set1 = list(set(line[\"author_source\"].split(\" \")))\n",
    "    set2 = list(set(line[\"author_target\"].split(\" \")))\n",
    "    \n",
    "    set1 = list(filter(lambda a: a != \"\" and a!=\" \", set1))\n",
    "    set2 = list(filter(lambda a: a != \"\" and a!=\" \", set2))\n",
    "    \n",
    "    count = 0\n",
    "    for word in set1:\n",
    "        for word2 in set2:\n",
    "            if word in word2 or word2 in word:\n",
    "                count += 1\n",
    "                break\n",
    "    return count\n",
    "\n",
    "\n",
    "training_set[\"author_nb_common\"] = training_set.progress_apply(nb_common_author,axis=1)\n",
    "testing_set[\"author_nb_common\"] = testing_set.progress_apply(nb_common_author,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_common_author(line):\n",
    "    set1 = list(set(line[\"author_source\"].split(\" \")))\n",
    "    set2 = list(set(line[\"author_target\"].split(\" \")))\n",
    "    \n",
    "    set1 = list(filter(lambda a: a != \"\" and a!=\" \", set1))\n",
    "    set2 = list(filter(lambda a: a != \"\" and a!=\" \", set2))\n",
    "    \n",
    "    for word in set1:\n",
    "        for word2 in set2:\n",
    "            if word in word2 or word2 in word:\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "training_set[\"author_is_one_common\"] = training_set.progress_apply(one_common_author,axis=1)\n",
    "testing_set[\"author_is_one_common\"] = testing_set.progress_apply(one_common_author,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nb_common_classification(line):\n",
    "    if line[\"classification_source\"]!=line[\"classification_source\"]:\n",
    "        return 0\n",
    "    if line[\"classification_target\"]!=line[\"classification_target\"]:\n",
    "        return 0\n",
    "    set1 = list(set(line[\"classification_source\"].split(\".\")))\n",
    "    set2 = list(set(line[\"classification_target\"].split(\".\")))\n",
    "    \n",
    "    set1 = list(filter(lambda a: a != \"\" and a!=\" \", set1))\n",
    "    set2 = list(filter(lambda a: a != \"\" and a!=\" \", set2))\n",
    "    count = 0\n",
    "    \n",
    "    for word in set1:\n",
    "        for word2 in set2:\n",
    "            if (word in word2 or word2 in word):\n",
    "                count += 1\n",
    "                break\n",
    "    return 2*count/(len(set1)+len(set2))\n",
    "\n",
    "training_set[\"common_classification\"] = training_set.progress_apply(nb_common_classification,axis=1)\n",
    "testing_set[\"common_classification\"] = testing_set.progress_apply(nb_common_classification,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_common_word(line):\n",
    "    set1 = set(line[\"title_source\"].split(\" \"))\n",
    "    set2 = set(line[\"title_target\"].split(\" \"))\n",
    "    set1 = set(filter(lambda a: a != \"\" and a!=\" \", set1))\n",
    "    set2 = set(filter(lambda a: a != \"\" and a!=\" \", set2))\n",
    "    return 1 if len(set1 & set2)>0 else 0\n",
    "\n",
    "training_set[\"title_is_one_common\"] = training_set.progress_apply(one_common_word,axis=1)\n",
    "testing_set[\"title_is_one_common\"] = testing_set.progress_apply(one_common_word,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nb_common_word(line):\n",
    "    set1 = set(line[\"title_source\"].split(\" \"))\n",
    "    set2 = set(line[\"title_target\"].split(\" \"))\n",
    "    set1 = set(filter(lambda a: a != \"\" and a!=\" \", set1))\n",
    "    set2 = set(filter(lambda a: a != \"\" and a!=\" \", set2))\n",
    "    return len(set1 & set2)\n",
    "\n",
    "training_set[\"title_nb_common_word\"] = training_set.progress_apply(nb_common_word,axis=1)\n",
    "testing_set[\"title_nb_common_word\"] = testing_set.progress_apply(nb_common_word,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nb_target_cited(line):\n",
    "    return(inlinks[line[\"pos_target\"]])\n",
    "\n",
    "training_set[\"nb_target_cited\"] = training_set.progress_apply(nb_target_cited,axis=1)\n",
    "testing_set[\"nb_target_cited\"] = testing_set.progress_apply(nb_target_cited,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set.to_csv(path_to_data+\"improved_training_set.csv\")\n",
    "testing_set.to_csv(path_to_data+\"improved_testing_set.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
