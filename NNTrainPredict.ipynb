{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Embedding, Dropout, Conv1D, Conv2D, MaxPooling1D, Dense, Merge, Flatten\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.merge import concatenate\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.regularizers import l2\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_optimizer = 'adam'\n",
    "N_VECTORIZATION = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_data = \"data/\"\n",
    "\n",
    "training_set = pd.read_csv(path_to_data+\"improved_training_set.csv\")\n",
    "testing_set = pd.read_csv(path_to_data+\"improved_testing_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_features = [\"description_d2v_\"+str(i)+\"_source\" for i in range(N_VECTORIZATION)]+[\"description_d2v_\"+str(i)+\"_target\" for i in range(N_VECTORIZATION)]\n",
    "selected_features += [\"title_d2v_\"+str(i)+\"_source\" for i in range(N_VECTORIZATION)]+[\"title_d2v_\"+str(i)+\"_target\" for i in range(N_VECTORIZATION)]\n",
    "selected_features += [\"common_neighbor\",\n",
    "                     \"same_cluster\",\n",
    "                     \"jaccard\",\n",
    "                     \"diff_in_bc\",\n",
    "                     \"diff_in_inlinks\",\n",
    "                     \"diff_in_year\",\n",
    "                     \"author_nb_common\",\n",
    "                     \"author_is_one_common\",\n",
    "                     \"common_classification\",\n",
    "                     \"title_is_one_common\",\n",
    "                     \"title_nb_common_word\",\n",
    "                     \"cos_similarity_title\",\n",
    "                     \"cos_similarity_description\",\n",
    "                     \"target_eccentricty\",\n",
    "                     'inlinks_target',\n",
    "                     'betweenness_author_target',\n",
    "                     'inlinks_author_target',\n",
    "                     'cos_similarity_tf_title',\n",
    "                     'cos_similarity_tf_description',\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = training_set.fillna(0)\n",
    "training_set[selected_features_global] = preprocessing.scale(training_set[selected_features])\n",
    "\n",
    "testing_set = testing_set.fillna(0)\n",
    "testing_set[selected_features_global] = preprocessing.scale(testing_set[selected_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr = training_set[selected_features_global].as_matrix()[10000:]\n",
    "Y_tr = training_set[\"label\"].as_matrix()[10000:]\n",
    "\n",
    "X_val = training_set[selected_features_global].as_matrix()[:10000]\n",
    "Y_val = training_set[\"label\"].as_matrix()[:10000]\n",
    "\n",
    "X_tst = testing_set[selected_features_global].as_matrix()\n",
    "\n",
    "del(training_set, testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1500, activation = \"relu\", input_dim=len(selected_features_global)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1500, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=my_optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[f1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"modelsNN/weights_1500_1500.{epoch:02d}-{f1:.4f}-{val_f1:.4f}.hdf5\", monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "model.fit(X_tr,Y_tr,batch_size=128,epochs=50,verbose=1,validation_data=(X_val, Y_val,), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(X_tst)\n",
    "predicted = [int(i>0.5) for i in predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame(predicted,columns=[\"category\"])\n",
    "\n",
    "df_sub.to_csv('output_NN.csv', float_format='%.6f', index_label=\"ID\")\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
